#
# Production environment overrides for the `kube‑prometheus‑stack` dependency.
#
# These settings mirror those in `values-dev.yaml` but are intended for
# production clusters.  All components (Prometheus, Alertmanager, Grafana,
# Node Exporter and kube-state-metrics) are enabled.  Alertmanager routes
# critical alerts to both Mattermost and email using the provided webhook
# URL and SMTP credentials.  Grafana is provisioned with a Prometheus
# datasource and a basic node resource dashboard.  Custom recording and
# alerting rules detect high CPU and memory usage on nodes.

kube-prometheus-stack:
  crds:
    enabled: true

  prometheus:
    enabled: true
    service:
      port: 9090
      targetPort: 9090

  alertmanager:
    enabled: true
    service:
      port: 9093
      targetPort: 9093
    config:
      global:
        resolve_timeout: 5m
        smtp_smarthost: smtp.mailgun.org:587
        smtp_from: connect-dev@linkafrica.org
        smtp_auth_username: connect-dev@linkafrica.org
        apiKey: "${MAILGUN_API_KEY}"
        
        smtp_require_tls: true
      route:
        group_by:
          - namespace
          - alertname
          - severity
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: mattermost-and-email
        routes:
          - matchers:
              - alertname="Watchdog"
            receiver: mattermost-and-email
      inhibit_rules:
        - source_matchers:
            - severity="critical"
          target_matchers:
            - severity=~"warning|info"
          equal:
            - namespace
            - alertname
        - source_matchers:
            - severity="warning"
          target_matchers:
            - severity="info"
          equal:
            - namespace
            - alertname
        - source_matchers:
            - alertname="InfoInhibitor"
          target_matchers:
            - severity="info"
          equal:
            - namespace
      receivers:
        - name: mattermost-and-email
          webhook_configs:
            - url: https://mattermost.edusc.us/hooks/93wdijzsspd7fc55gy8f8mhpcc
              send_resolved: true
          email_configs:
            - to: engineering@edusuc.net
              send_resolved: true
      templates:
        - /etc/alertmanager/config/*.tmpl

  grafana:
    enabled: true
    service:
      port: 3000
    adminUser: admin
    adminPassword: admin
    sidecar:
      datasources:
        enabled: true
        defaultDatasourceEnabled: false
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
    # Configure the Prometheus datasource.  The service created by
    # kube‑prometheus‑stack is named `prom-stack-kube-prometheus-prometheus`
    # (without a duplicated `stack`).  If the URL here is incorrect
    # Grafana will be unable to query Prometheus and dashboards will
    # display no data.
    additionalDataSources:
      - name: Prometheus
        type: prometheus
        access: proxy
        # Use the operator-created headless service for Prometheus.  This
        # eliminates the need to know the exact service name generated by
        # the sub‑chart and ensures Grafana can always reach Prometheus.
        url: http://prometheus-operated:9090
        isDefault: true
        jsonData:
          timeInterval: "30s"
        version: 1
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: custom
            orgId: 1
            folder: 'Custom Dashboards'
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/custom
    dashboards:
      custom:
        node-resources:
          json: |
            {
              "schemaVersion": 30,
              "title": "Node Resource Dashboard",
              "uid": "node-resources-dashboard",
              "time": { "from": "now-1h", "to": "now" },
              "panels": [
                {
                  "type": "timeseries",
                  "title": "CPU utilisation by node",
                  "datasource": "Prometheus",
                  "targets": [
                    {
                      "expr": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
                      "legendFormat": "{{ '{{' }} instance {{ '}}' }}"
                    }
                  ]
                },
                {
                  "type": "timeseries",
                  "title": "Memory utilisation by node",
                  "datasource": "Prometheus",
                  "targets": [
                    {
                      "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
                      "legendFormat": "{{ '{{' }} instance {{ '}}' }}"
                    }
                  ]
                }
              ]
            }

  kubeStateMetrics:
    enabled: true
  nodeExporter:
    enabled: true
    service:
      port: 9100

  additionalPrometheusRulesMap:
    custom-rules:
      groups:
        - name: custom-recording
          rules:
            - record: cluster:node_cpu:avg_rate5m
              expr: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) by (instance)
        - name: custom-alerts
          rules:
            - alert: HighNodeCPUUsage
              expr: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) by (instance) > 0.85
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "High CPU usage detected on node {{ '{{' }} $labels.instance {{ '}}' }}"
                description: "CPU usage on node {{ '{{' }} $labels.instance {{ '}}' }} has been above 85% for more than 5 minutes."
            - alert: HighNodeMemoryUsage
              expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.85
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "High memory utilisation detected on node {{ '{{' }} $labels.instance {{ '}}' }}"
                description: "Memory utilisation on node {{ '{{' }} $labels.instance {{ '}}' }} has been above 85% for more than 5 minutes."

  kubeApiServer:
    enabled: true
  kubelet:
    enabled: true
  kubeControllerManager:
    enabled: true
  kubeScheduler:
    enabled: true
  coreDns:
    enabled: true
  kubeProxy:
    enabled: true